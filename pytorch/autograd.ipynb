{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37fd9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "# single image with 3 channels, height and width of 64\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21196710",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data) # forward pass through the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b1e59c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward() # backpropagation to calculate weight gradients\n",
    "# stores the gradients for each model parameter in the parameters .grad attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdb2c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9df7c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() # do gradient descent, the optimizer will adjust each parameter by its gradient stored in .grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fe46d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# differentiation in autograd\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f35c870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([-72., -32.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "external_grad = torch.tensor([1., 1.]) # we need to explicitly pass a gradient argument in q.backward because its a vector\n",
    "# gradient is a tensor of the same shape as Q and it represents the gradient of Q wrt itself\n",
    "Q.backward(gradient=external_grad)\n",
    "# check if gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57031d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients?: False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "# torch autograd tracks operations on all tensors which have requires_grad set to True, for tensors that dont require gradients,\n",
    "# setting this attribute to false excludes it from the gradient computation DAG\n",
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "# the output tensor of an operation will require gradients if only a single input tensor has requires_grad=True\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients?: {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "372562aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in NN, parameters that dont compute gradients are called frozen parameters\n",
    "from torch import nn, optim\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "# freeze all parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72cb64fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In resnet, the classifier is the last linear layer model.fc\n",
    "# we can simply replace it with a new linear layer (unfrozen by default) that acts as our classifier\n",
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "690defce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all parameters in the model, except for the parameters of model.fc are frozen\n",
    "# the only parameters that compute the gradients are the weights and bias of model.fc\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b09979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
