{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "a964854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "907498d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract all names from names.txt\n",
    "names = open(\"names.txt\", \"r\")\n",
    "names = names.read().split('\\n')\n",
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "3ff5c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all trigrams as tuples\n",
    "trigrams = []\n",
    "for word in names:\n",
    "    word = ['.'] + list(word) + ['.']\n",
    "    # combines multiple iterables element-by-element into tuples\n",
    "    # zip(iterable1, iterable2, iterable3)\n",
    "    for ch1, ch2, ch3 in zip(word, word[1:], word[2:]):\n",
    "        trigrams.append((ch1, ch2, ch3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "b8772c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset where first 2 characters in the tuple are inputs (x), last character is output (y)\n",
    "inputs = [ch1 + ch2 for (ch1, ch2, _ch3) in trigrams]\n",
    "labels = [ch3 for (_ch1, _ch2, ch3) in trigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "5cc0f127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# convert characters to integer mappings\n",
    "N = 27\n",
    "chars = (sorted(set(''.join(inputs))))\n",
    "bigrams = []\n",
    "for ch1 in chars:\n",
    "    for ch2 in chars:\n",
    "        bigrams.append(ch1 + ch2)\n",
    "\n",
    "input_to_i = {bigram: i for i, bigram in enumerate(bigrams)}\n",
    "output_to_i = {ch: i for i, ch in enumerate(chars)}\n",
    "i_to_output = {i: ch for i, ch in enumerate(chars)}\n",
    "print(i_to_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "10a900f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# one hot encode inputs\n",
    "x = [input_to_i[input] for input in inputs]\n",
    "x = torch.tensor(x)\n",
    "x = F.one_hot(x, num_classes=N*N).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "1a7ee075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural net that maps one_hot encoded inputs to a probability distribution for the next character in the sequence\n",
    "W = torch.randn((N*N, N), requires_grad=True)\n",
    "y = torch.tensor([output_to_i[ch] for ch in labels], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e07e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2989392280578613\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "for epoch in range(1000):\n",
    "    # forward pass\n",
    "    logits = x @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "    # calculate loss\n",
    "    loss = -probs[torch.arange(len(probs)), y].log().mean() + 0.1*(W**2).mean()\n",
    "    # backpropagate\n",
    "    loss.backward()\n",
    "    #optimize, dont add to comp graph for this calculation\n",
    "    with torch.no_grad():\n",
    "        W -= 10*W.grad\n",
    "        W.grad.zero_()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "164e839c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brah.\n",
      "vana.\n",
      "palan.\n",
      "olhlvee.\n",
      "bewjoric.\n",
      "areyanne.\n",
      "yosellarian.\n",
      "xaurggia.\n",
      "vin.\n",
      "chen.\n",
      "yanichay.\n",
      "esfq.\n",
      "belie.\n",
      "jeon.\n",
      "man.\n",
      "vine.\n",
      "ca.\n",
      "pree.\n",
      "quan.\n",
      "xbbiah.\n",
      "quaryah.\n",
      "ya.\n",
      "yanshipmrqfyn.\n",
      "vi.\n",
      "ter.\n",
      "coraveren.\n",
      "chea.\n",
      "vanne.\n",
      "sulin.\n",
      "uyw.\n",
      "vienihilee.\n",
      "va.\n",
      "carldyn.\n",
      "na.\n",
      "cswxmwwcznrona.\n",
      "rah.\n",
      "en.\n",
      "isen.\n",
      "xafftkuwashiya.\n",
      "na.\n",
      "taid.\n",
      "qsffydena.\n",
      "no.\n",
      "vi.\n",
      "yia.\n",
      "valee.\n",
      "hayan.\n",
      "gavon.\n",
      "vippcra.\n",
      "esea.\n",
      "mon.\n",
      "vio.\n",
      "jaqoe.\n",
      "xai.\n",
      "cani.\n",
      "parstray.\n",
      "vytdurie.\n",
      "th.\n",
      "maana.\n",
      "ca.\n",
      "xzaxlvan.\n",
      "vane.\n",
      "britlley.\n",
      "phannellia.\n",
      "zyre.\n",
      "xr.\n",
      "orrish.\n",
      "sha.\n",
      "wenn.\n",
      "via.\n",
      "iz.\n",
      "chia.\n",
      "valliraymiyarmjcfey.\n",
      "yavibbie.\n",
      "wavi.\n",
      "xanie.\n",
      "ilyn.\n",
      "vexkjcfcrzelian.\n",
      "wane.\n",
      "dellexjlmjhus.\n",
      "xaanarahmarmon.\n",
      "yanny.\n",
      "vistilana.\n",
      "bettenna.\n",
      "marayah.\n",
      "vellandawn.\n",
      "yah.\n",
      "yanna.\n",
      "uina.\n",
      "vvalyanjudtw.\n",
      "xwkyzuzarsquin.\n",
      "caster.\n",
      "esal.\n",
      "lingez.\n",
      "sebwuandzn.\n",
      "yannel.\n",
      "vgyttlynna.\n",
      "bree.\n",
      "sadea.\n",
      "jane.\n"
     ]
    }
   ],
   "source": [
    "for name in range(100):    \n",
    "    out = []\n",
    "    bigram = '..'\n",
    "    ix = 1\n",
    "    T = 0.8\n",
    "    while True:\n",
    "        # get probability distribution using forward pass through neural net\n",
    "        xenc = F.one_hot(torch.tensor([input_to_i[bigram]]), num_classes=N*N).float()\n",
    "        logits = (xenc @ W)\n",
    "        if bigram != '..':\n",
    "            logits = logits / T\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(dim=1, keepdim=True)\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
    "        out.append(i_to_output[ix])\n",
    "        bigram = bigram[1] + i_to_output[ix]\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
